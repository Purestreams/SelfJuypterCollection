{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (40 marks)\n",
    "\n",
    "In Q1, we will be focusing on the IMDb dataset. This is a dataset for binary sentiment classification, and is provided with a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. To load the dataset, you can easily download the dataset by adding this line in your colab notebook:\n",
    "\n",
    "```\n",
    "! wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XP3tOMpe1S18",
    "outputId": "b6c42ff5-0bc6-4e23-999b-e0d7b9a96154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-29 19:12:15--  http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  7.25MB/s    in 19s     \n",
      "\n",
      "2024-11-29 19:12:34 (4.24 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the Large IMDB Movie Review Dataset\n",
    "# the task is binary classification: positive or negative review\n",
    "\n",
    "!wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fLOjrIu1S19"
   },
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries including torch, numpy, and os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HB1dzrZJ1S19"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from collections import namedtuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 4011\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3n7VMPc1S19"
   },
   "source": [
    "# Set Random Seed and Device\n",
    "Set the random seed for reproducibility and define the device for computation (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axc43REZ1S19",
    "outputId": "83f2178c-d8ef-46f4-ddc6-c9341d9f5d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 4011\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device to MPS (Metal Performance Shaders) if available, otherwise use CUDA or CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JV5sII3X1S1-"
   },
   "source": [
    "# Load IMDb Dataset\n",
    "Load the IMDb dataset and read the movie reviews from the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6-TmkWp1S1-",
    "outputId": "8795ed9a-74cb-446f-d4a4-c4af2b97b172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load IMDb Dataset\n",
    "train_path = \"aclImdb/train/\"\n",
    "test_path = \"aclImdb/test/\"\n",
    "\n",
    "# Define a namedtuple to store sentences\n",
    "Sentence = namedtuple('Sentence', ['index', 'tokens', 'label'])\n",
    "\n",
    "def read_imdb_movie_dataset(dataset_path):\n",
    "    indices = []\n",
    "    text = []\n",
    "    rating = []\n",
    "    i = 0\n",
    "\n",
    "    # Read positive reviews\n",
    "    for filename in os.listdir(os.path.join(dataset_path, \"pos\")):\n",
    "        file_path = os.path.join(dataset_path, \"pos\", filename)\n",
    "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(1)\n",
    "        i += 1\n",
    "\n",
    "    # Read negative reviews\n",
    "    for filename in os.listdir(os.path.join(dataset_path, \"neg\")):\n",
    "        file_path = os.path.join(dataset_path, \"neg\", filename)\n",
    "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        indices.append(i)\n",
    "        text.append(data)\n",
    "        rating.append(0)\n",
    "        i += 1\n",
    "\n",
    "    sentences = [Sentence(index, text.split(), rating) for index, text, rating in zip(indices, text, rating)]\n",
    "    return sentences\n",
    "\n",
    "# Load train and test datasets\n",
    "train_examples = read_imdb_movie_dataset(train_path)\n",
    "test_examples = read_imdb_movie_dataset(test_path)\n",
    "\n",
    "# Print the number of examples in train and test datasets\n",
    "print(f\"Number of training examples: {len(train_examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAsdXhka1S1-"
   },
   "source": [
    "# Preprocess Data\n",
    "Preprocess the text data including tokenization and building vocabulary with special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryQMdm1H1S1-",
    "outputId": "89246389-2b92-4144-cf10-c6a2180b07e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown vocab size: 424424\n",
      "Vocab size: 22521\n",
      "Vocab size: 2\n",
      "Source vocab size: 22521\n",
      "Target vocab size: 2\n"
     ]
    }
   ],
   "source": [
    "# Define special tokens\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "# Define VocabItem class\n",
    "class VocabItem:\n",
    "    def __init__(self, string, hash=None):\n",
    "        self.string = string\n",
    "        self.count = 0\n",
    "        self.hash = hash\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'VocabItem({})'.format(self.string)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Define Vocab class\n",
    "class Vocab:\n",
    "    def __init__(self, min_count=0, no_unk=False, add_padding=False, add_bos=False, add_eos=False, unk=None):\n",
    "        self.no_unk = no_unk\n",
    "        self.vocab_items = []\n",
    "        self.vocab_hash = {}\n",
    "        self.word_count = 0\n",
    "        self.special_tokens = []\n",
    "        self.min_count = min_count\n",
    "        self.add_padding = add_padding\n",
    "        self.add_bos = add_bos\n",
    "        self.add_eos = add_eos\n",
    "        self.unk = unk\n",
    "\n",
    "        self.UNK = None\n",
    "        self.PAD = None\n",
    "        self.BOS = None\n",
    "        self.EOS = None\n",
    "\n",
    "        self.index2token = []\n",
    "        self.token2index = {}\n",
    "\n",
    "        self.finished = False\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        if self.finished:\n",
    "            raise RuntimeError('Vocabulary is finished')\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in self.vocab_hash:\n",
    "                self.vocab_hash[token] = len(self.vocab_items)\n",
    "                self.vocab_items.append(VocabItem(token))\n",
    "\n",
    "            self.vocab_items[self.vocab_hash[token]].count += 1\n",
    "            self.word_count += 1\n",
    "\n",
    "    def finish(self):\n",
    "        token2index = self.token2index\n",
    "        index2token = self.index2token\n",
    "\n",
    "        tmp = []\n",
    "\n",
    "        if not self.no_unk:\n",
    "            if self.unk:\n",
    "                self.UNK = VocabItem(self.unk, hash=0)\n",
    "                self.UNK.count = self.vocab_items[self.vocab_hash[self.unk]].count\n",
    "                index2token.append(self.UNK)\n",
    "                self.special_tokens.append(self.UNK)\n",
    "\n",
    "                for token in self.vocab_items:\n",
    "                    if token.string != self.unk:\n",
    "                        tmp.append(token)\n",
    "            else:\n",
    "                self.UNK = VocabItem(UNK, hash=0)\n",
    "                index2token.append(self.UNK)\n",
    "                self.special_tokens.append(self.UNK)\n",
    "\n",
    "                for token in self.vocab_items:\n",
    "                    if token.count <= self.min_count:\n",
    "                        self.UNK.count += token.count\n",
    "                    else:\n",
    "                        tmp.append(token)\n",
    "        else:\n",
    "            for token in self.vocab_items:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token: token.count, reverse=True)\n",
    "\n",
    "        if self.add_bos:\n",
    "            self.BOS = VocabItem(BOS)\n",
    "            tmp.append(self.BOS)\n",
    "            self.special_tokens.append(self.BOS)\n",
    "\n",
    "        if self.add_eos:\n",
    "            self.EOS = VocabItem(EOS)\n",
    "            tmp.append(self.EOS)\n",
    "            self.special_tokens.append(self.EOS)\n",
    "\n",
    "        if self.add_padding:\n",
    "            self.PAD = VocabItem(PAD)\n",
    "            tmp.append(self.PAD)\n",
    "            self.special_tokens.append(self.PAD)\n",
    "\n",
    "        index2token += tmp\n",
    "\n",
    "        for i, token in enumerate(self.index2token):\n",
    "            token2index[token.string] = i\n",
    "            token.hash = i\n",
    "\n",
    "        self.index2token = index2token\n",
    "        self.token2index = token2index\n",
    "\n",
    "        if not self.no_unk:\n",
    "            print('Unknown vocab size:', self.UNK.count)\n",
    "\n",
    "        print('Vocab size: %d' % len(self))\n",
    "\n",
    "        self.finished = True\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.index2token[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index2token)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.index2token)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.token2index\n",
    "\n",
    "    def tokens2indices(self, tokens, add_bos=False, add_eos=False):\n",
    "        string_seq = []\n",
    "        if add_bos:\n",
    "            string_seq.append(self.BOS.hash)\n",
    "        for token in tokens:\n",
    "            if self.no_unk:\n",
    "                string_seq.append(self.token2index[token])\n",
    "            else:\n",
    "                string_seq.append(self.token2index.get(token, self.UNK.hash))\n",
    "        if add_eos:\n",
    "            string_seq.append(self.EOS.hash)\n",
    "        return string_seq\n",
    "\n",
    "    def indices2tokens(self, indices, ignore_ids=()):\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx in ignore_ids:\n",
    "                continue\n",
    "            tokens.append(self.index2token[idx].string)\n",
    "        return tokens\n",
    "\n",
    "# Initialize vocabularies\n",
    "src_vocab = Vocab(min_count=10, add_padding=True)\n",
    "tgt_vocab = Vocab(no_unk=True, add_padding=False)\n",
    "\n",
    "# Add tokens to vocabularies\n",
    "for sentence in train_examples:\n",
    "    src_vocab.add_tokens(sentence.tokens[:300])\n",
    "    tgt_vocab.add_tokens([sentence.label])\n",
    "\n",
    "# Finish building vocabularies\n",
    "src_vocab.finish()\n",
    "tgt_vocab.finish()\n",
    "\n",
    "# Print vocab sizes\n",
    "print(f\"Source vocab size: {len(src_vocab)}\")\n",
    "print(f\"Target vocab size: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUji8feC1S1-"
   },
   "source": [
    "# Build Vocabulary\n",
    "Build the vocabulary from the preprocessed text data and print the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQ7kuAz41S1-",
    "outputId": "5508d3d5-c6c6-4abd-b570-95c6a94f1e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown vocab size: 424424\n",
      "Vocab size: 22521\n",
      "Vocab size: 2\n",
      "Source vocab size: 22521\n",
      "Target vocab size: 2\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "# Initialize vocabularies\n",
    "src_vocab = Vocab(min_count=10, add_padding=True)\n",
    "tgt_vocab = Vocab(no_unk=True, add_padding=False)\n",
    "\n",
    "# Add tokens to vocabularies\n",
    "for sentence in train_examples:\n",
    "    src_vocab.add_tokens(sentence.tokens[:300])\n",
    "    tgt_vocab.add_tokens([sentence.label])\n",
    "\n",
    "# Finish building vocabularies\n",
    "src_vocab.finish()\n",
    "tgt_vocab.finish()\n",
    "\n",
    "# Print vocab sizes\n",
    "print(f\"Source vocab size: {len(src_vocab)}\")\n",
    "print(f\"Target vocab size: {len(tgt_vocab)}\")\n",
    "\n",
    "Vocabs = namedtuple('Vocabs', ['src', 'tgt'])\n",
    "vocabs = Vocabs(src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71_iqRlO1S1_"
   },
   "source": [
    "# Create Embedding Matrix\n",
    "Create an embedding matrix based on the vocabulary and print its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lnXOwTW1S1_",
    "outputId": "dac090c6-ed8f-42f1-b49f-9d793261447b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22521, 300])\n"
     ]
    }
   ],
   "source": [
    "# Create Embedding Matrix\n",
    "embedding_size = 300\n",
    "\n",
    "# Create an embedding matrix based on the vocabulary\n",
    "embeddings = nn.Embedding(\n",
    "    len(src_vocab),\n",
    "    embedding_size,\n",
    "    padding_idx=src_vocab.PAD.hash\n",
    ")\n",
    "\n",
    "# Print the size of the embedding matrix\n",
    "print(embeddings.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q1-1. To get your data prepared, build up Pytorch dataloaders for model training and print out one batch of training data. (15 marks)\n",
    "\n",
    "- To check whether your dataloader can work successfully, you can choose to use `next(iter(train_dataloader))`. You can refer to https://pytorch.org/tutorials/beginner/basics/data_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIQBRsuC1S1_",
    "outputId": "9ecc2bab-7126-4229-a761-63351db36cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <__main__.BatchTuple object at 0x7c0527cc17e0>, 'tgt': array([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
      "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]), '_is_torch': False}\n"
     ]
    }
   ],
   "source": [
    "# Build PyTorch Dataloaders\n",
    "\n",
    "# Define Batch and BatchTuple classes\n",
    "class Batch(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Batch, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        self._is_torch = False\n",
    "\n",
    "    def to_torch_(self, device):\n",
    "        self._is_torch = False\n",
    "        for key in self.keys():\n",
    "            value = self[key]\n",
    "            if isinstance(value, BatchTuple):\n",
    "                value.to_torch_(device)\n",
    "            if isinstance(value, np.ndarray):\n",
    "                self[key] = torch.from_numpy(value).to(device)\n",
    "\n",
    "class BatchTuple(object):\n",
    "    def __init__(self, sequences, lengths, sublengths, masks):\n",
    "        self.sequences = sequences\n",
    "        self.lengths = lengths\n",
    "        self.sublengths = sublengths\n",
    "        self.masks = masks\n",
    "        self._is_torch = False\n",
    "\n",
    "    def to_torch_(self, device):\n",
    "        if not self._is_torch:\n",
    "            self.sequences = torch.tensor(\n",
    "                self.sequences, device=device, dtype=torch.long\n",
    "            )\n",
    "            if self.lengths is not None:\n",
    "                self.lengths = torch.tensor(\n",
    "                    self.lengths, device=device, dtype=torch.long\n",
    "                )\n",
    "            if self.sublengths is not None:\n",
    "                self.sublengths = torch.tensor(\n",
    "                    self.sublengths, device=device, dtype=torch.long\n",
    "                )\n",
    "            if self.masks is not None:\n",
    "                self.masks = torch.tensor(\n",
    "                    self.masks, device=device, dtype=torch.float\n",
    "                )\n",
    "\n",
    "# Define padding function\n",
    "def pad_list(sequences, dim0_pad=None, dim1_pad=None, align_right=False, pad_value=0):\n",
    "    sequences = [np.asarray(sublist) for sublist in sequences]\n",
    "    if not dim0_pad:\n",
    "        dim0_pad = len(sequences)\n",
    "    if not dim1_pad:\n",
    "        dim1_pad = max(len(seq) for seq in sequences)\n",
    "    out = np.full(shape=(dim0_pad, dim1_pad), fill_value=pad_value)\n",
    "    lengths = []\n",
    "    for i in range(len(sequences)):\n",
    "        data_length = len(sequences[i])\n",
    "        lengths.append(data_length)\n",
    "        offset = dim1_pad - data_length if align_right else 0\n",
    "        np.put(out[i], range(offset, offset + data_length), sequences[i])\n",
    "    lengths = np.array(lengths)\n",
    "    return out, lengths\n",
    "\n",
    "# Define SequenceClassificationBatchBuilder class\n",
    "class SequenceClassificationBatchBuilder(object):\n",
    "    def __init__(self, vocabs, max_len=None):\n",
    "        self.vocabs = vocabs\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        sequences = [example.tokens[:self.max_len] for example in examples]\n",
    "        labels = [example.label for example in examples]\n",
    "        sequences = [self.vocabs.src.tokens2indices(seq) for seq in sequences]\n",
    "        sequences, lengths = pad_list(sequences, pad_value=self.vocabs.src.PAD.hash)\n",
    "        labels = [self.vocabs.tgt.token2index[label] for label in labels]\n",
    "        batch = Batch(\n",
    "            src=BatchTuple(sequences, lengths, None, None),\n",
    "            tgt=np.array(labels)\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_examples, batch_size=100, shuffle=True, collate_fn=SequenceClassificationBatchBuilder(vocabs, 300))\n",
    "test_dataloader = DataLoader(test_examples, batch_size=100, shuffle=False, collate_fn=SequenceClassificationBatchBuilder(vocabs, 300))\n",
    "\n",
    "# Print one batch of training data\n",
    "one_batch = next(iter(train_dataloader))\n",
    "print(one_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-2. We choose bidirectional LSTM (BiLSTM) as the model. Train the model for 5 epoches with embedding matrix you obtained earlier, and for each epoch, print out the training loss, training accuracy, testing loss and testing accuracy. You could choose any appropriate loss function and values for hyperparameters. (25 marks)\n",
    "\n",
    "- If you found difficulty understanding the structure of BiLSTM, you may refer to the supplementary note named *notes_on_lstm* inside tutorial 9 for detailed information.\n",
    "\n",
    "- You definitely want to use GPU for this colab notebook. Go to Edit > Notebook settings as the following: Click on “Notebook settings” and select “GPU”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YA3A2ToF1S1_"
   },
   "outputs": [],
   "source": [
    "# Define BiLSTM Model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, n_layers, dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=src_vocab.PAD.hash)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lengths = lengths.cpu().to(torch.int64)  # Move lengths to CPU and ensure it is int64\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = embedding_size\n",
    "hidden_dim = 256\n",
    "output_size = 1\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Instantiate the model\n",
    "model = BiLSTM(embedding_dim, hidden_dim, len(src_vocab), output_size, n_layers, dropout).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TS-p-8Kd1S1_",
    "outputId": "17a94642-3b3f-414d-840b-bd8c85b1f09b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:37<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6674, Train Accuracy: 0.5836\n",
      "Test Loss: 0.5548, Test Accuracy: 0.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:37<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.5602, Train Accuracy: 0.7118\n",
      "Test Loss: 0.4748, Test Accuracy: 0.7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:36<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.4639, Train Accuracy: 0.7844\n",
      "Test Loss: 0.5096, Test Accuracy: 0.7592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.3849, Train Accuracy: 0.8295\n",
      "Test Loss: 0.3838, Test Accuracy: 0.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:37<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.3348, Train Accuracy: 0.8540\n",
      "Test Loss: 0.4802, Test Accuracy: 0.8177\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Train and Evaluate BiLSTM Model\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch.to_torch_(device)\n",
    "        predictions = model(batch.src.sequences, batch.src.lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.tgt.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(predictions))\n",
    "        correct += (predicted == batch.tgt).sum().item()\n",
    "        total += batch.tgt.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss / len(train_dataloader), accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch.to_torch_(device)\n",
    "            predictions = model(batch.src.sequences, batch.src.lengths).squeeze(1)\n",
    "            loss = criterion(predictions, batch.tgt.float())\n",
    "            epoch_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(predictions))\n",
    "            correct += (predicted == batch.tgt).sum().item()\n",
    "            total += batch.tgt.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss / len(test_dataloader), accuracy\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_model(model, train_dataloader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate_model(model, test_dataloader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (50 marks)\n",
    "### Implement the idea in paper ***A Neural Probabilistic Language Model*** (https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) to train a trigram model. We will use the brown corpus in nltk package as the dataset. Train the model for 5 epoches and print out the training loss, training accuracy, testing loss, and testing accuracy. You can use these codes to download the corpus:\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/michaelzhu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in Brown corpus: 1161192\n",
      "First 10 words in Brown corpus: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Download the Brown corpus using NLTK\n",
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Prepare the corpus with all words\n",
    "brown_words = brown.words()\n",
    "print(f\"Total number of words in Brown corpus: {len(brown_words)}\")\n",
    "print(f\"First 10 words in Brown corpus: {brown_words[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in Brown corpus: 56057\n",
      "Most common words: [('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n",
      "Vocabulary size: 56057\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create term frequency of the words\n",
    "term_freq = Counter(brown_words)\n",
    "print(f\"Total unique words in Brown corpus: {len(term_freq)}\")\n",
    "print(f\"Most common words: {term_freq.most_common(10)}\")\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = {word: idx for idx, (word, _) in enumerate(term_freq.items())}\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 928952\n",
      "Number of development samples: 232238\n"
     ]
    }
   ],
   "source": [
    "# Create Training and Development Sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert words to indices based on the vocabulary\n",
    "word_indices = [vocabulary[word] for word in brown_words]\n",
    "\n",
    "# Create trigrams\n",
    "trigrams = [(word_indices[i], word_indices[i+1], word_indices[i+2]) for i in range(len(word_indices) - 2)]\n",
    "\n",
    "# Split the data into training and development sets (80% training, 20% development)\n",
    "train_data, dev_data = train_test_split(trigrams, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of development samples: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramNNModel(\n",
      "  (embeddings): Embedding(56057, 100)\n",
      "  (linear1): Linear(in_features=200, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=56057, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Trigram Neural Network Model\n",
    "class TrigramNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(TrigramNNModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = torch.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 100\n",
    "context_size = 2  # Trigram context size is 2\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TrigramNNModel(vocab_size, embedding_dim, context_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "# Define Negative Log-Likelihood Loss\n",
    "\n",
    "# Implement the negative log-likelihood loss function\n",
    "def negative_log_likelihood_loss(output, target):\n",
    "    return loss_function(output, target)\n",
    "\n",
    "# test\n",
    "output = torch.tensor([[0.1, 0.2, 0.7]], requires_grad=True)  # Example output\n",
    "target = torch.tensor([2])  # Example target\n",
    "\n",
    "loss = negative_log_likelihood_loss(output, target)1\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save Model\n",
    "Train the model for 5 epochs and save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928952/928952 [2:19:16<00:00, 111.17it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 6.7277, Accuracy: 0.1097\n",
      "Development Loss: 6.5272, Development Accuracy: 0.1188\n"
     ]
    }
   ],
   "source": [
    "# Train and Save Model\n",
    "from tqdm import tqdm\n",
    "# Train the model for 5 epochs\n",
    "num_epochs = 1\n",
    "\n",
    "print(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    for context1, context2, target in tqdm(train_data):\n",
    "        context_tensor = torch.tensor([context1, context2], dtype=torch.long)\n",
    "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = negative_log_likelihood_loss(log_probs, target_tensor)\n",
    "\n",
    "        # Backward pass and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(log_probs, 1)\n",
    "        correct_predictions += (predicted == target_tensor).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    accuracy = correct_predictions / len(train_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"trigram_nn_model.pth\")\n",
    "\n",
    "# Evaluate on the development set\n",
    "model.eval()\n",
    "dev_loss = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context1, context2, target in dev_data:\n",
    "        context_tensor = torch.tensor([context1, context2], dtype=torch.long)\n",
    "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = negative_log_likelihood_loss(log_probs, target_tensor)\n",
    "        dev_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(log_probs, 1)\n",
    "        correct_predictions += (predicted == target_tensor).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy for the development set\n",
    "avg_dev_loss = dev_loss / len(dev_data)\n",
    "dev_accuracy = correct_predictions / len(dev_data)\n",
    "\n",
    "print(f\"Development Loss: {avg_dev_loss:.4f}, Development Accuracy: {dev_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time is too long for this task, so choose to train the model for fewer epochs (e.g., 1 epoch) to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928952/928952 [14:32<00:00, 1064.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.3482, Training Accuracy: 0.1207\n",
      "Development Loss: 6.5272, Development Accuracy: 0.1188\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "# Evaluate on the training set\n",
    "model.eval()\n",
    "train_loss = 0\n",
    "train_correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context1, context2, target in tqdm(train_data):\n",
    "        context_tensor = torch.tensor([context1, context2], dtype=torch.long)\n",
    "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = negative_log_likelihood_loss(log_probs, target_tensor)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(log_probs, 1)\n",
    "        train_correct_predictions += (predicted == target_tensor).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy for the training set\n",
    "avg_train_loss = train_loss / len(train_data)\n",
    "train_accuracy = train_correct_predictions / len(train_data)\n",
    "\n",
    "print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on the development set\n",
    "dev_loss = 0\n",
    "dev_correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context1, context2, target in dev_data:\n",
    "        context_tensor = torch.tensor([context1, context2], dtype=torch.long)\n",
    "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = negative_log_likelihood_loss(log_probs, target_tensor)\n",
    "        dev_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(log_probs, 1)\n",
    "        dev_correct_predictions += (predicted == target_tensor).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy for the development set\n",
    "avg_dev_loss = dev_loss / len(dev_data)\n",
    "dev_accuracy = dev_correct_predictions / len(dev_data)\n",
    "\n",
    "print(f\"Development Loss: {avg_dev_loss:.4f}, Development Accuracy: {dev_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the chatglm-4 API, and write a proper prompt using prompt engineering knowledge to let chatglm perform the task correctly:\n",
    "\n",
    "``Take the last letters of the words and concatenate them.``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-AZBU9o2PPyP8POkghDwEUkZIQETVD\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"sr, yek, ot, odo, ar, gnp\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1732948993,\n",
      "  \"model\": \"gpt-35-turbo\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 14,\n",
      "    \"prompt_tokens\": 51,\n",
      "    \"total_tokens\": 65\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "\n",
    "client = AzureOpenAI(  \n",
    "    api_key=\"XXX\",\n",
    "    api_version=\"2024-02-01\",  \n",
    "    azure_endpoint=\"https://XXX.openai.azure.com/\"  \n",
    ")  \n",
    "\n",
    "deployment_name = \"gpt-35-turbo\"  \n",
    "\n",
    "# Define the words list\n",
    "words_list = ['Linius Victor', 'strawberry cake', 'Nice headshot', 'Cristiano Ronaldo', 'Brawl Star', 'Natural Language Processing']\n",
    "\n",
    "sentence = \"Take the last letters of the words and concatenate them: \" + \", \".join(words_list)\n",
    "\n",
    "chat_prompt = [\n",
    "{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"you are a helpful assistant\"\n",
    "},\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": sentence\n",
    "}\n",
    "]  \n",
    "\n",
    "# Include speech result if speech is enabled  \n",
    "speech_result = chat_prompt  \n",
    "\n",
    "# Generate the completion  \n",
    "completion = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages=speech_result,  \n",
    "    max_tokens=800,  \n",
    "    temperature=0.2,  \n",
    "    top_p=0.95,  \n",
    "    frequency_penalty=0,  \n",
    "    presence_penalty=0,  \n",
    "    stop=None,  \n",
    "    stream=False  \n",
    ")  \n",
    "    \n",
    "print(completion.to_json())  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
